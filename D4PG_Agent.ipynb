{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from collections import deque\n",
    "from Models import actor\n",
    "from Models import critic\n",
    "from Categorical_Distributions import projected_prob_batch2_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_states = 33, n_actions = 4, actor_hidden = 50, \n",
    "                 critic_hidden = 300, seed = 0, roll_out = 5, replay_buffer_size = 1e6, \n",
    "                 replay_batch = 128, lr_actor = 5e-5,  lr_critic = 5e-5, epsilon = 0.3, \n",
    "                 tau = 1e-3,  gamma = 1, update_interval = 4, noise_fn = np.random.normal, \n",
    "                 vmin = -10, vmax = 10, n_atoms = 51):\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.actor_hidden = actor_hidden # hidden nodes in the 1st layer of actor network\n",
    "        self.critic_hidden = critic_hidden # hidden nodes in the 1st layer of critic network\n",
    "        self.seed = seed\n",
    "        self.roll_out = roll_out # roll out steps for n-step bootstrap; taken to be same as in D4PG paper\n",
    "        self.replay_buffer = replay_buffer_size\n",
    "        self.replay_batch = replay_batch # batch of memories to sample during training\n",
    "        self.lr_actor = lr_actor # this was taken to same as the value in the D4PG paper for hard tasks\n",
    "        self.lr_critic = lr_critic # taken from the D4PG paper\n",
    "        self.epsilon = epsilon # to scale the noise before mixing with the actions; same as in D4PG paper\n",
    "        self.tau = tau # for soft updates of the target networks\n",
    "        self.gamma = gamma # do not decrease this below 1\n",
    "        # note that we want the reacher to stay in goal position as long as possible\n",
    "        # thus keeping gamma = 1 will ecourage the agent to increase its holding time\n",
    "        self.update_every = update_interval # steps between successive updates\n",
    "        self.noise = noise_fn # noise function; \n",
    "        # Note D4PG paper reported that \n",
    "        # using normal distribution instead of OU noise does not affect performance\n",
    "        # will also experiment with OU noise if the need arises\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.n_atoms = n_atoms\n",
    "        self.delta = (vmax - vmin)/(n_atoms - 1)\n",
    "        self.zi = torch.linspace(self.vmin, self.vmax, self.n_atoms).view(-1,1).to(device)\n",
    "        # in numpy using linspace is much slower than the following way of doing it\n",
    "        # but in torch its a little bit faster \n",
    "        # I guess that this is due to the time it takes to convert from numpy to torch tensors\n",
    "        # self.zi = torch.from_numpy(np.array([ vmin + ii*self.delta for ii in range(self.n_atoms)])).view(-1,1).float().to(device)\n",
    "        \n",
    "        # discounts to be applied at each step of roll_out\n",
    "        self.discounts = torch.tensor([self.gamma**powr \n",
    "                                       for powr in range(self.roll_out - 1 )]).double().view(-1,1).to(device)\n",
    "        \n",
    "        self.local_actor = actor(self.n_states, self.n_actions, self.actor_hidden, self.seed).to(device)\n",
    "        # output of local critic network should be log_softmax\n",
    "        self.local_critic = critic(self.n_states, self.n_actions, self.n_atoms, \n",
    "                                   self.critic_hidden, self.seed, output = 'logprob').to(device)\n",
    "        \n",
    "        self.target_actor = actor(self.n_states, self.n_actions, self.actor_hidden, self.seed).to(device)\n",
    "        # target critic should output probabilities\n",
    "        self.target_critic = critic(self.n_states, self.n_actions, self.n_atoms, \n",
    "                                    self.critic_hidden, self.seed, output = 'prob').to(device)\n",
    "        \n",
    "        # initialize target_actor and target_critic weights to be \n",
    "        # the same as the corresponding local networks\n",
    "        for target_c_params, local_c_params in zip(self.target_critic.parameters(), \n",
    "                                                   self.local_critic.parameters()):\n",
    "            target_c_params.data.copy_(local_c_params.data)\n",
    "        \n",
    "        for target_a_params, local_a_params in zip(self.target_actor.parameters(), \n",
    "                                                   self.local_actor.parameters()):\n",
    "            target_a_params.data.copy_(local_a_params.data)\n",
    "            \n",
    "        # optimizers for the local actor and local critic\n",
    "        self.actor_optim = torch.optim.Adam(self.local_actor.parameters(), lr = self.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.local_critic.parameters(), lr = self.lr_critic)\n",
    "        \n",
    "        # loss function\n",
    "        self.criterion = nn.KLDivLoss(reduction = 'batchmean')\n",
    "        \n",
    "        # steps counter to keep track of steps passed between updates\n",
    "        self.t_step = 0\n",
    "        \n",
    "        # replay memory \n",
    "        self.memory = ReplayBuffer(self.replay_buffer, self.n_states, \n",
    "                                   self.n_actions, self.roll_out)\n",
    "    \n",
    "    def act(self, states):\n",
    "        # convert states to a torch tensor and move to the device\n",
    "        # for the multiagent case we will get a batch of states \n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.local_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.local_actor(states).cpu().detach().numpy()\n",
    "            noise = self.noise(size = actions.shape)\n",
    "            actions = np.clip(actions + noise, -1, 1)\n",
    "        self.local_actor.train()\n",
    "        return actions\n",
    "            \n",
    "    def step(self, new_memories):\n",
    "        # new memories is a batch of tuples\n",
    "        # each tuple consists of (n-1)-steps of state, action, reward, done and the n-state\n",
    "        # here n is the roll_out length\n",
    "        self.memory.add(new_memories)\n",
    "        \n",
    "        # update the networks after every self.update_every steps\n",
    "        # make sure to check that the replay_buffer has enough memories\n",
    "        self.t_step = (self.t_step+1)%self.update_every\n",
    "        if self.t_step == 0 and self.memory.__len__() > 2*self.replay_batch:\n",
    "            self.learn()\n",
    "    \n",
    "    def learn(self):\n",
    "        # sample a batch of memories from the replay buffer\n",
    "        states_0, actions_0, rewards, states_fin = self.memory.sample(self.replay_batch)\n",
    "        \n",
    "        states_0 = torch.from_numpy(states_0).float().to(device)\n",
    "        actions_0 = torch.from_numpy(actions_0).float().to(device)\n",
    "        states_fin = torch.from_numpy(states_fin).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).to(device)\n",
    "        \n",
    "        # get an action for the n-th state from the target actor\n",
    "        self.target_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions_fin = self.target_actor(states_fin)\n",
    "        self.target_actor.train()    \n",
    "        \n",
    "        # get the Q-value for the n-th state and action from the target critic\n",
    "        self.target_critic.eval()\n",
    "        with torch.no_grad():\n",
    "            # target critic directly outputs the probabilities \n",
    "            target_probs = self.target_critic(states_fin, actions_fin)\n",
    "        self.target_critic.train() \n",
    "        \n",
    "        # Compute the TD-target for the n-step bootstrap\n",
    "        # discounts = np.array([self.gamma**powr for powr in range(self.roll_out - 1 )])\n",
    "        n_step_rewards = torch.matmul(rewards, self.discounts) # sum of the discounted rewards collected during the roll_out\n",
    "        # n_step_rewards = torch.from_numpy(n_step_rewards).float().to(device)\n",
    "        # fin_done = None # was the final state a terminal state?\n",
    "        # projected_probs = n_step_rewards + (self.gamma**(self.roll_out -1))*fin_Qs\n",
    "        projected_probs = projected_prob_batch2_torch(self.vmin, self.vmax, self.n_atoms, \n",
    "                                               self.gamma**(self.roll_out -1), n_step_rewards,  \n",
    "                                               target_probs, self.replay_batch)\n",
    "        # projected_probs = torch.from_numpy(projected_probs).float().to(device)\n",
    "        \n",
    "        # train the local critic\n",
    "        self.critic_optim.zero_grad()\n",
    "        # get a Q-value for the beginning state and action from the local critic\n",
    "        local_log_probs = self.local_critic(states_0, actions_0)\n",
    "        # compute the local critic's loss\n",
    "        loss_c = self.criterion(local_log_probs, projected_probs)\n",
    "        # can I just write loss_c = - torch.sum(projected_probs*local_log_probs)/self.replay_batch\n",
    "        loss_c.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # train the local actor\n",
    "        self.actor_optim.zero_grad()\n",
    "        # get the local_action for the initial state\n",
    "        local_a = self.local_actor(states_0)\n",
    "        # get the Q_value for the initial state and local_a\n",
    "        # this gives the actor's loss\n",
    "        # apply torch.exp() to convert the critic's output into probabilities from log_prob\n",
    "        probs = torch.exp(self.local_critic(states_0, local_a))\n",
    "        loss_a = - torch.matmul(probs, self.zi).mean()\n",
    "        loss_a.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        # apply soft updates to the target network\n",
    "        self.update_target_networks()\n",
    "      \n",
    "    def update_target_networks(self):\n",
    "        # update target actor\n",
    "        for params_target, params_local in zip(self.target_actor.parameters(),\n",
    "                                               self.local_actor.parameters()):\n",
    "            updates = (1.0-self.tau)*params_target.data + self.tau*params_local.data \n",
    "            params_target.data.copy_(updates)\n",
    "            \n",
    "        # update target critic \n",
    "        for params_target, params_local in zip(self.target_critic.parameters(), \n",
    "                                               self.local_critic.parameters()):\n",
    "            updates = (1.0-self.tau)*params_target.data + self.tau*params_local.data \n",
    "            params_target.data.copy_(updates)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, buffer_size, n_states, n_actions, roll_out):\n",
    "        self.memory = deque(maxlen = int(buffer_size))\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.roll_out = roll_out\n",
    "            \n",
    "    def add(self, experience_windows):\n",
    "        \n",
    "        for window in experience_windows:\n",
    "            self.memory.append(window)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = sample(self.memory, batch_size)\n",
    "        \n",
    "        # from the above batch obtain states at 0th step, action at 0th step, \n",
    "        # rewards for the n-1 subsequent steps i.e. rewards until and incuding the penultimate step\n",
    "        # and the states at the last roll_out step\n",
    "        \n",
    "        states_0 = []\n",
    "        actions_0 = []\n",
    "        rewards = []\n",
    "        states_fin = []\n",
    "        for exp in batch:\n",
    "            state_0 = exp[0][:self.n_states]\n",
    "            action_0 = exp[0][self.n_states:self.n_states+self.n_actions]\n",
    "            reward = [exp[ii][self.n_states+self.n_actions] for ii in range(self.roll_out-1)]\n",
    "            state_fin = exp[self.roll_out-1][:self.n_states]\n",
    "            states_0.append(state_0)\n",
    "            actions_0.append(action_0)\n",
    "            rewards.append(reward)\n",
    "            states_fin.append(state_fin)\n",
    "        \n",
    "        states_0 = np.array(states_0)\n",
    "        actions_0 = np.array(actions_0)\n",
    "        rewards = np.array(rewards)\n",
    "        states_fin = np.array(states_fin)\n",
    "        \n",
    "        return states_0, actions_0, rewards, states_fin\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
