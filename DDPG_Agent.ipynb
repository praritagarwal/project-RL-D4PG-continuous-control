{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import actor\n",
    "from Models import critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_states = 33, n_actions = 4, actor_hidden = 100, \n",
    "                 critic_hidden = 100, seed = 0, roll_out = 5, replay_buffer_size = 1e6, \n",
    "                 replay_batch = 128, lr_actor = 5e-5,  lr_critic = 5e-5, epsilon = 0.3, \n",
    "                 tau = 1e-3,  gamma = 1, update_interval = 4, noise_fn = np.random.normal):\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.actor_hidden = actor_hidden # hidden nodes in the 1st layer of actor network\n",
    "        self.critic_hidden = critic_hidden # hidden nodes in the 1st layer of critic network\n",
    "        self.seed = seed\n",
    "        self.roll_out = roll_out # roll out steps for n-step bootstrap; taken to be same as in D4PG paper\n",
    "        self.replay_buffer = replay_buffer_size\n",
    "        self.replay_batch = replay_batch # batch of memories to sample during training\n",
    "        self.lr_actor = lr_actor # this was taken to same as the value in the D4PG paper for hard tasks\n",
    "        self.lr_critic = lr_critic # taken from the D4PG paper\n",
    "        self.epsilon = epsilon # to scale the noise before mixing with the actions; same as in D4PG paper\n",
    "        self.tau = tau # for soft updates of the target networks\n",
    "        self.gamma = gamma # do not decrease this below 1\n",
    "        # note that we want the reacher to stay in goal position as long as possible\n",
    "        # thus keeping gamma = 1 will ecourage the agent to increase its holding time\n",
    "        self.update_every = update_interval # steps between successive updates\n",
    "        self.noise = noise_fn # noise function; \n",
    "        # Note D4PG paper reported that \n",
    "        # using normal distribution instead of OU noise does not affect performance\n",
    "        # will also experiment with OU noise if the need arises\n",
    "        \n",
    "        \n",
    "        self.local_actor = actor(self.n_states, self.n_actions, self.actor_hidden, self.seed).to(device)\n",
    "        self.local_critic = critic(self.n_states, self.n_actions, self.critic_hidden, self.seed).to(device)\n",
    "        \n",
    "        self.target_actor = actor(self.n_states, self.n_actions, self.actor_hidden, self.seed).to(device)\n",
    "        self.target_critic = critic(self.n_states, self.n_actions, self.critic_hidden, self.seed).to(device)\n",
    "        \n",
    "        # initialize target_actor and target_critic weights to be \n",
    "        # the same as the corresponding local networks\n",
    "        for target_c_params, local_c_params in zip(self.target_critic.parameters(), \n",
    "                                                   self.local_critic.parameters()):\n",
    "            target_c_params.data.copy_(local_c_params.data)\n",
    "        \n",
    "        for target_a_params, local_a_params in zip(self.target_actor.parameters(), \n",
    "                                                   self.local_actor.parameters()):\n",
    "            target_a_params.data.copy_(local_a_params.data)\n",
    "            \n",
    "        # optimizers for the local actor and local critic\n",
    "        self.actor_optim = torch.optim.Adam(self.local_actor.parameters(), lr = self.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.local_critic.parameters(), lr = self.lr_critic)\n",
    "        \n",
    "        # loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # steps counter to keep track of steps passed between updates\n",
    "        self.t_steps = 0\n",
    "        \n",
    "        # replay memory \n",
    "        self.memory = ReplayBuffer()\n",
    "    \n",
    "    def act(self, states):\n",
    "        # convert states to a torch tensor and move to the device\n",
    "        # for the multiagent case we will get a batch of states \n",
    "        states = torch.from_numpy(states).to(device).float()\n",
    "        self.local_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.local_actor(states).cpu().detach().numpy()\n",
    "            noise = self.noise(size = actions.shape)\n",
    "            actions = actions + noise\n",
    "        self.local_actor.train()\n",
    "        return actions\n",
    "            \n",
    "    def step(self, new_memories):\n",
    "        # new memories is a batch of tuples\n",
    "        # each tuple consists of (n-1)-steps of state, action, reward, done and the n-state\n",
    "        # here n is the roll_out length\n",
    "        self.memory.add(new_memories)\n",
    "        \n",
    "        # update the networks after every self.update_every steps\n",
    "        # make sure to check that the replay_buffer has enough memories\n",
    "        self.t_step = (self.t_step+1)%self.update_every\n",
    "        if self.t_step == 0 and self.memory.__len__ > 2*self.replay_batch:\n",
    "            self.learn()\n",
    "    \n",
    "    def learn(self):\n",
    "        # sample a batch of memories from the replay buffer\n",
    "        \n",
    "        # get an action for the n-th state from the target actor\n",
    "        self.target_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            fin_action = None\n",
    "        self.target_actor.train()    \n",
    "        \n",
    "        # get the Q-value for the n-th state and action from the target critic\n",
    "        self.target_critic.eval()\n",
    "        with torch.no_grad():\n",
    "            fin_Q = None\n",
    "        self.target_critic.train()    \n",
    "            \n",
    "        \n",
    "        # Compute the TD-target for the n-step bootstrap\n",
    "        n_step_rewards = None # sum of the discounted rewards collected during the roll_out\n",
    "        fin_done = None # was the final state a terminal state?\n",
    "        target_Q = n_step_rewards + (self.gamma**(self.roll_out -1))*fin_Q*(1-fin_done)\n",
    "        \n",
    "        # train the local critic\n",
    "        self.critic_optim.zero_grad()\n",
    "        # get a Q-value for the beginning state and action from the local critic\n",
    "        local_Q = None\n",
    "        # compute the local critic's loss\n",
    "        loss_c = self.criterion(local_Q, target_Q)\n",
    "        loss_c.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # train the local actor\n",
    "        self.actor_optim.zero_grad()\n",
    "        # get the local_action for the initial state\n",
    "        local_a = None\n",
    "        # get the Q_value for the initial state and local_a\n",
    "        # this gives the actor's loss\n",
    "        loss_actor = None # this should be: - self.local_critic(initial_state, local_a)\n",
    "        loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        # apply soft updates to the target network\n",
    "        self.update_target_networks()\n",
    "      \n",
    "    def update_target_networks(self):\n",
    "        # update target actor\n",
    "        for params_target, params_local in zip(self.target_actor.parameters(),\n",
    "                                               self.local_actor.parameters()):\n",
    "            updates = (1.0-self.tau)*params_target.data + self.tau*params_local.data \n",
    "            params_target.data.copy_(updates)\n",
    "            \n",
    "        # update target critic \n",
    "        for params_target, params_local in zip(self.target_critic.parameters(), \n",
    "                                               self.target_actor.parameters()):\n",
    "            updates = (1.0-self.tau)*params_target.data + self.tau*params_local.data \n",
    "            params_target.data.copy_(updates)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def add(self):\n",
    "        pass\n",
    "    \n",
    "    def sample(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
