{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import actor\n",
    "from Models import critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_states = 33, n_actions = 4, actor_hidden = 100, \n",
    "                 critic_hidden = 100, seed = 0, roll_out = 5, replay_buffer_size = 1e6, \n",
    "                 replay_batch = 128, lr_actor = 5e-5,  lr_critic = 5e-5, epsilon = 0.3, \n",
    "                 tau = 1e-3,  gamma = 1, update_interval = 4, noise_fn = np.random.normal):\n",
    "        \n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.actor_hidden = actor_hidden # hidden nodes in the 1st layer of actor network\n",
    "        self.critic_hidden = critic_hidden # hidden nodes in the 1st layer of critic network\n",
    "        self.seed = seed\n",
    "        self.roll_out = roll_out # roll out steps for n-step bootstrap; taken to be same as in D4PG paper\n",
    "        self.replay_buffer = replay_buffer_size\n",
    "        self.replay_batch = replay_batch # batch of memories to sample during training\n",
    "        self.lr_actor = lr_actor # this was taken to same as the value in the D4PG paper for hard tasks\n",
    "        self.lr_critic = lr_critic # taken from the D4PG paper\n",
    "        self.epsilon = epsilon # to scale the noise before mixing with the actions; same as in D4PG paper\n",
    "        self.tau = tau # for soft updates of the target networks\n",
    "        self.gamma = gamma # do not decrease this below 1\n",
    "        # note that we want the reacher to stay in goal position as long as possible\n",
    "        # thus keeping gamma = 1 will ecourage the agent to increase its holding time\n",
    "        self.update_every = update_interval # steps between successive updates\n",
    "        self.noise = noise_fn # noise function; \n",
    "        # Note D4PG paper reported that \n",
    "        # using normal distribution instead of OU noise does not affect performance\n",
    "        # will also experiment with OU noise if the need arises\n",
    "        \n",
    "        \n",
    "        self.local_actor = actor(self.n_states, self.n_actions, self.actor_hidden, self.seed).to(device)\n",
    "        self.local_critic = critic(self.n_states, self.n_actions, self.critic_hidden, self.seed).to(device)\n",
    "        \n",
    "        self.target_actor = actor(self.n_states, self.n_actions, self.actor_hidden, self.seed).to(device)\n",
    "        self.target_critic = critic(self.n_states, self.n_actions, self.critic_hidden, self.seed).to(device)\n",
    "        \n",
    "        # initialize target_actor and target_critic weights to be \n",
    "        # the same as the corresponding local networks\n",
    "        for target_c_params, local_c_params in zip(self.target_critic.parameters(), \n",
    "                                                   self.local_critic.parameters()):\n",
    "            target_c_params.data.copy_(local_c_params.data)\n",
    "        \n",
    "        for target_a_params, local_a_params in zip(self.target_actor.parameters(), \n",
    "                                                   self.local_actor.parameters()):\n",
    "            target_a_params.data.copy_(local_a_params.data)\n",
    "            \n",
    "        # optimizers for the local actor and local critic\n",
    "        self.actor_optim = torch.optim.Adam(self.local_actor.parameters(), lr = self.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.local_critic.parameters(), lr = self.lr_critic)\n",
    "        \n",
    "        # steps counter to keep track of steps passed between updates\n",
    "        self.t_steps = 0\n",
    "        \n",
    "        # replay memory \n",
    "        self.memory = ReplayBuffer()\n",
    "    \n",
    "    def act(self, states):\n",
    "        # convert states to a torch tensor and move to the device\n",
    "        # for the multiagent case we will get a batch of states \n",
    "        states = torch.from_numpy(states).to(device).float()\n",
    "        self.local_actor.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.local_actor(states).cpu().detach().numpy()\n",
    "            noise = self.noise(size = actions.shape)\n",
    "            actions = actions + noise\n",
    "        self.local_actor.train()\n",
    "        return actions\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def add(self):\n",
    "        pass\n",
    "    \n",
    "    def sample(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
