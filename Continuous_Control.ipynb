{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For single agent Reacher_NoVis [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux_NoVis.zip)\n",
    "\n",
    "For multiagent Reacher_NoVis [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='env2/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.13149999706074594\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the environment is setup such that it necessarily monitors the agents for 1000 steps and then terminates. Thus all the agents get a done at exactly the same time. This can also be verified by explicitly counting the number of steps executed before the episode ends. This always comes out to be 1000. Let us see this explicitly in the following code cell (we will also tweak the condition to the end the episode to be given by when all the agents are done, though this will not matter): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0874999980442226\n",
      "number of steps executed in the episode: 1001\n",
      "Total score (averaged over agents) this episode: 0.12599999718368055\n",
      "number of steps executed in the episode: 1001\n",
      "Total score (averaged over agents) this episode: 0.08399999812245369\n",
      "number of steps executed in the episode: 1001\n",
      "Total score (averaged over agents) this episode: 0.1114999975077808\n",
      "number of steps executed in the episode: 1001\n",
      "Total score (averaged over agents) this episode: 0.14699999671429395\n",
      "number of steps executed in the episode: 1001\n"
     ]
    }
   ],
   "source": [
    "# count the number of steps in 5 episodes\n",
    "def check_num_steps(num_episodes = 5):\n",
    "    for _ in range(num_episodes):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        steps = 0\n",
    "        while True:\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            steps+=1\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.all(dones): \n",
    "                break\n",
    "        print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "        print('number of steps executed in the episode: {}'.format(steps))\n",
    "\n",
    "check_num_steps()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the flow of training will be as follows:\n",
    "\n",
    "Note that episodes come to an end after a fixed time. Therefore there is no terminal state in the game. Hence, there are no actions so bad taht they will lead the agent to a terminal state causing it to finish earlier than it should. The only categorization of actions is whether they help it maintain its position in the goal or not. Had the time not run out, the action could have scored better or worse. In this way I feel that this task is similar to being a continuous task with the hard-cut off on time making it episodic. I therefore feel that we should not use dones for evaluating the target value in the TD-step. In concurrence with this idea, I will only collect experiences where the neither of the states were the last states of that episode. This also implies that I will not include dones in the memory\n",
    "\n",
    "    repeat for num_episodes\n",
    "      reset the env.\n",
    "      obtain the initial states\n",
    "      for the first roll_out - 1 steps in each episode\n",
    "        obtain actions for the current states\n",
    "        obtain the rewards \n",
    "        obtain dones\n",
    "        concatenate the current states, actions, and rewards \n",
    "        push each row of the concatenated array into its corresponding deque\n",
    "        obtain the next states\n",
    "        states = next_states\n",
    "   \n",
    "      for all subsequent steps in each episode i.e. until done\n",
    "        obtain actions for the current states\n",
    "        obtain the rewards \n",
    "        obtain dones\n",
    "        if not done then \n",
    "           concatenate the current states, actions and rewards \n",
    "           push each row of the concatenated array into its corresponding deque\n",
    "           pass the list of deques to the agent\n",
    "           agent takes a step to put the experiences in the buffer and learn\n",
    "           obtain the next states\n",
    "           states = next_states\n",
    "        else\n",
    "           break to start a new episode\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DDPG_Agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.21\n",
      "Episode 200\tAverage Score: 3.61\n",
      "Episode 300\tAverage Score: 6.72\n",
      "Episode 400\tAverage Score: 21.15\n",
      "Episode 475\tAverage Score: 30.00\n",
      "Environment solved in 375 episodes!\tAverage Score: 30.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ddpg(n_episodes=2000, roll_out = 5):\n",
    "    \"\"\"DDPG\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    ep_scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    experience_windows = [deque(maxlen = roll_out) for _ in range(num_agents)]\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations            # get the current state\n",
    "        scores = np.zeros(num_agents)\n",
    "        for step in range(roll_out-1):\n",
    "            actions = agent.act(states) # select an action (for each agent)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            #dones = env_info.local_done                        # see if episode finished\n",
    "            #if np.any(dones):\n",
    "            #    break\n",
    "            single_step_mem = np.concatenate((states, \n",
    "                                              actions,\n",
    "                                              np.array(rewards).reshape(-1,1)), \n",
    "                                             axis = 1)\n",
    "            for idx in range(num_agents):\n",
    "                experience_windows[idx].append(single_step_mem[idx])\n",
    "        \n",
    "            scores += rewards                         # update the score (for each agent)\n",
    "            states = next_states\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]        # send the action to the environment\n",
    "            next_states = env_info.vector_observations      # get the next state\n",
    "            rewards = env_info.rewards                      # get the reward\n",
    "            dones = env_info.local_done                     # see if episode has finished\n",
    "            scores += rewards\n",
    "            if np.any(dones):                               # if finished then move to next episode\n",
    "                break\n",
    "            single_step_mem = np.concatenate((states, \n",
    "                                              actions,\n",
    "                                              np.array(rewards).reshape(-1,1)), \n",
    "                                             axis = 1)\n",
    "            for idx in range(num_agents):\n",
    "                experience_windows[idx].append(single_step_mem[idx])\n",
    "                \n",
    "            agent.step(experience_windows)\n",
    "            states = next_states\n",
    "        \n",
    "        scores_window.append(np.mean(scores))       # save most recent scores averaged over the agents\n",
    "        ep_scores.append(np.mean(scores))           # save most recent scores averaged over the agents\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            # print('memory size: {}'.format(len(agent.memory)))\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save({'local_actor': agent.local_actor.state_dict(), \n",
    "                        'local_critic': agent.local_critic.state_dict()}, 'checkpoint.pth')\n",
    "            break\n",
    "    return ep_scores\n",
    "\n",
    "agent = Agent(n_states = state_size, n_actions = action_size, roll_out = 5,  \n",
    "              update_interval = 3)\n",
    "scores = ddpg(n_episodes = 5000, roll_out = 5)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we implemented a small variation of the [DDPG](https://arxiv.org/abs/1509.02971) algorithm. These variations are a subset of those implemented in the [D4PG](https://arxiv.org/pdf/1804.08617.pdf) algorithm. \n",
    "\n",
    "Before going into the details of the different variations let us quickly describe the architecture of the actor and the critic networks. This are provided in the [Models](Models.py) module\n",
    " \n",
    "The actor network: This has 5 layers which are as follows\n",
    "\n",
    " - layer 1: input features = 33, output features = 100, activation = selu\n",
    " - layer 2: input features = 100, output features = 50, activation = selu\n",
    " - layer 3: input features = 50, output features = 25, activation = selu\n",
    " - layer 4: input features = 25, output features = 12, activation = selu\n",
    " - layer 5: input features = 12, output features = 4, activation = tanh\n",
    " \n",
    " The critic network: This has 5 layers which are \n",
    " \n",
    " - layer 1: input features = 37, output features = 100, activation = selu\n",
    " - layer 2: input features = 100, output features = 50, activation = selu\n",
    " - layer 3: input features = 50, output features = 25, activation = selu\n",
    " - layer 4: input features = 25, output features = 12, activation = selu\n",
    " - layer 5: input features = 12, output features = 1, activation = None\n",
    " \n",
    "Now, one of the first things that we did different from the classic DDPG agent is that instead of adding the Ornstein-Uhlenbeck (OU) noise to the agent's actions, we simple added a small amount of Gaussion noise. This is based on the observation made in the D4PG paper that suggests that replacing the OU noise with the Gaussian noise has little to no effect on the performance of the agent. \n",
    "\n",
    "Secondly, instead of updating the agent based on a single TD step, we used the [N-step bootstrap](https://arxiv.org/abs/1602.01783). Just like in the D4PG paper we chose N = 5. \n",
    "\n",
    "Note that unlike the D4PG paper, we did not implement a priority reply. Also, again unlike the D4PG paper, our critic did not try to the model the distribution of Q-values but the Q-values themself. \n",
    "\n",
    "Finally, we used the following hyperparameters (almost all of these except the batch_size are based on the values in the D4PG paper):\n",
    "\n",
    "- replay_buffer_size = 1e6\n",
    "- replay_batch = 128\n",
    "- lr_actor = 5e-5\n",
    "- lr_critic = 5e-5\n",
    "- epsilon = 0.3 \n",
    "- tau = 1e-3\n",
    "- gamma = 1\n",
    "- update_interval = 3\n",
    "- noise = Gaussion with a mean of 0 and std. dev. 1\n",
    "\n",
    "Here ```epsilon``` is the factor by which we scale the Gaussian noise before adding it to the agent's actions.\n",
    "\n",
    "\n",
    "We used the Adam optimizer for both the agent and the critic.\n",
    "\n",
    "Our agent learned to solve the environment in 375 episodes. \n",
    "\n",
    "Note that this is a little larger than the number of episodes taken by the benchmark implementation discussed by Udacity's team. However, we will like to point out that from looking at our training graph it is clear that the agent can achieve a much much better score if it is allowed to train further, our training graph having a very nice positive gradient throughout. On the otherhand, the training graph for the benchmark implementation (as can be seen [here](graphs/UdacityBenchmarkImplementation.png)) seems to have plateaued by the time it is able to solve the environment. We thus believe that our agent would have performed significantly better then the benchmark had the avg. score expected from the agent been higher.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DDPG agent did a very good job at solving the environment. However, it will be interesting to experiment with techniques such as a prioritized replay buffer and applying the [distributional perpective](http://proceedings.mlr.press/v70/bellemare17a.html) to improve the agent as was done in the D4PG paper. Another interesting thing to try will be to modify the critic so as to learn the Advantage function rather than the Q-value function.\n",
    "\n",
    "On the otherhand, it will also be very interesting to compare this performance with other RL strategies such as the [PPO](https://arxiv.org/pdf/1707.06347.pdf) and the [Asynchronous methods](https://arxiv.org/abs/1602.01783) such as A3C etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
